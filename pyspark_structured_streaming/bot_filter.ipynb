{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "903750e6-9906-4902-8c39-69493e798362",
   "metadata": {},
   "source": [
    "## Adding Spark Dependencies\n",
    "* Kafka dependencies to connect to Kafka;\n",
    "* ABRIS dependencies to parse Kafka Schema Registry Avros;\n",
    "* Confluent Schema Registry Client to connect to Schema Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52332b0f-9e50-4bb7-8aa7-872bc5b02ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--repositories https://packages.confluent.io/maven --packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1,io.confluent:kafka-schema-registry-client:7.1.1,io.confluent:kafka-avro-serializer:7.1.1,za.co.absa:abris_2.12:6.3.0 pyspark-shell\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "kafka_deps = 'org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1'\n",
    "confluent_schema_deps = 'io.confluent:kafka-schema-registry-client:7.1.1,' + 'io.confluent:kafka-avro-serializer:7.1.1'\n",
    "abris_deps = 'za.co.absa:abris_2.12:6.3.0' \n",
    "spark_deps  = f\"--repositories https://packages.confluent.io/maven --packages {kafka_deps},{confluent_schema_deps},{abris_deps} pyspark-shell\"\n",
    "print(spark_deps)\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = spark_deps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262da166-0172-4cd5-94fe-b1278a72ac22",
   "metadata": {},
   "source": [
    "## Importing necessary libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c3f08d-47ff-47a8-90f8-52b0d77bb49d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, udf, struct\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.column import Column, _to_java_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26073696-e944-49fa-894a-a461f08cb1bc",
   "metadata": {},
   "source": [
    "## Starting Spark Context\n",
    "If you check your terminal you'll see that Spark is dowloading the libs from repositories (Maven and Confluent - added with ```--repositories``` flag) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa83255c-b61a-49f5-bb21-0384cd8d3eef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_context = SparkSession.builder.appName('processingBots').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e39392-0588-4f23-9cc0-da7fb6bcb066",
   "metadata": {},
   "source": [
    "## Creating functions to parse Confluent Avro\n",
    "Using examples from https://github.com/AbsaOSS/ABRiS/blob/master/documentation/python-documentation.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a5c243e-1a6a-4ce5-8732-48e047a8524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_avro(col, config):\n",
    "    \"\"\"\n",
    "    avro deserialize\n",
    "\n",
    "    :param col (PySpark column / str): column name \"key\" or \"value\"\n",
    "    :param config (za.co.absa.abris.config.FromAvroConfig): abris config, generated from abris_config helper function\n",
    "    :return: PySpark Column\n",
    "    \"\"\"\n",
    "    jvm_gateway = SparkContext._active_spark_context._gateway.jvm\n",
    "    abris_avro = jvm_gateway.za.co.absa.abris.avro\n",
    "\n",
    "    return Column(abris_avro.functions.from_avro(_to_java_column(col), config))\n",
    "\n",
    "def from_avro_abris_config(config_map, topic, is_key):\n",
    "    \"\"\"\n",
    "    Create from avro abris config with a schema url\n",
    "\n",
    "    :param config_map (dict[str, str]): configuration map to pass to deserializer, ex: {'schema.registry.url': 'http://localhost:8081'}\n",
    "    :param topic (str): kafka topic\n",
    "    :param is_key (bool): boolean\n",
    "    :return: za.co.absa.abris.config.FromAvroConfig\n",
    "    \"\"\"\n",
    "    jvm_gateway = SparkContext._active_spark_context._gateway.jvm\n",
    "    scala_map = jvm_gateway.PythonUtils.toScalaMap(config_map)\n",
    "\n",
    "    return jvm_gateway.za.co.absa.abris.config \\\n",
    "        .AbrisConfig \\\n",
    "        .fromConfluentAvro() \\\n",
    "        .downloadReaderSchemaByLatestVersion() \\\n",
    "        .andTopicNameStrategy(topic, is_key) \\\n",
    "        .usingSchemaRegistry(scala_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a277980-c898-46be-9690-2f621451c056",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Kafka Topics to Spark read Configs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5e7e787-834f-4aa9-92d7-b4b485b29e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_offsets = 'earliest'\n",
    "\n",
    "wikipedia_df = spark_context.readStream.format('kafka')\\\n",
    "    .option('kafka.bootstrap.servers', 'localhost:12091')\\\n",
    "    .option('subscribe', 'wikipedia.parsed')\\\n",
    "    .option(\"startingOffsets\", starting_offsets)\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "718e94da-9c6e-4d97-b78d-c49ae663e892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before parsing schema, this is the default schema from Kafka Topics\n",
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Before parsing schema, this is the default schema from Kafka Topics')\n",
    "wikipedia_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa208cd8-1c18-4762-9250-b3ab9df3fb7b",
   "metadata": {},
   "source": [
    "### Config Confluent Avro Parser using SSL and Basic Auth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "819f6f3a-18b0-4fd0-ba7f-e6a82b7b4d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "confluent_config = {\n",
    "    'schema.registry.url':'https://localhost:8085',\n",
    "    \"schema.registry.ssl.truststore.location\" :\"/home/jovyan/work/scripts/security/kafka.schemaregistry.truststore.jks\",\n",
    "    \"schema.registry.ssl.truststore.password\": 'confluent',\n",
    "    \"schema.registry.ssl.keystore.location\": '/home/jovyan/work/scripts/security/kafka.schemaregistry.keystore.jks',\n",
    "    \"schema.registry.ssl.keystore.password\" : \"confluent\",\n",
    "    \"basic.auth.credentials.source\": \"USER_INFO\",\n",
    "    \"basic.auth.user.info\": \"superUser:superUser\"\n",
    "} \n",
    "from_avro_abris_settings = from_avro_abris_config(confluent_config, 'wikipedia.parsed', False)\n",
    "wikipedia_df = wikipedia_df.withColumn('value', from_avro(col('value'), from_avro_abris_settings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbcac7a6-e417-4648-9a2d-8494d82d4735",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema after parsing:\n",
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- bot: boolean (nullable = true)\n",
      " |    |-- comment: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- length: struct (nullable = true)\n",
      " |    |    |-- new: long (nullable = true)\n",
      " |    |    |-- old: long (nullable = true)\n",
      " |    |-- log_action: string (nullable = true)\n",
      " |    |-- log_action_comment: string (nullable = true)\n",
      " |    |-- log_id: long (nullable = true)\n",
      " |    |-- log_type: string (nullable = true)\n",
      " |    |-- meta: struct (nullable = false)\n",
      " |    |    |-- domain: string (nullable = true)\n",
      " |    |    |-- dt: timestamp (nullable = false)\n",
      " |    |    |-- id: string (nullable = false)\n",
      " |    |    |-- request_id: string (nullable = true)\n",
      " |    |    |-- stream: string (nullable = false)\n",
      " |    |    |-- uri: string (nullable = true)\n",
      " |    |-- minor: boolean (nullable = true)\n",
      " |    |-- namespace: long (nullable = true)\n",
      " |    |-- parsedcomment: string (nullable = true)\n",
      " |    |-- patrolled: boolean (nullable = true)\n",
      " |    |-- revision: struct (nullable = true)\n",
      " |    |    |-- new: long (nullable = true)\n",
      " |    |    |-- old: long (nullable = true)\n",
      " |    |-- server_name: string (nullable = true)\n",
      " |    |-- server_script_path: string (nullable = true)\n",
      " |    |-- server_url: string (nullable = true)\n",
      " |    |-- timestamp: long (nullable = true)\n",
      " |    |-- title: string (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |    |-- user: string (nullable = true)\n",
      " |    |-- wiki: string (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Schema after parsing:')\n",
    "wikipedia_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f35d8-ac62-4009-a95a-84292be23404",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Filtering bots and sending to Topics\n",
    "Original Queries:\n",
    "\n",
    "\n",
    "No Bots\n",
    "```sql\n",
    "CREATE STREAM wikipedianobot AS SELECT *, (length->new - length->old) AS BYTECHANGE FROM wikipedia WHERE bot = false AND length IS NOT NULL AND length->new IS NOT NULL AND length->old IS NOT NULL;\n",
    "```\n",
    "Bots Query:\n",
    "```sql\n",
    "CREATE STREAM wikipediabot AS SELECT *, (length->new - length->old) AS BYTECHANGE FROM wikipedia WHERE bot = true AND length IS NOT NULL AND length->new IS NOT NULL AND length->old IS NOT NULL;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3af82109-f5b4-4afc-9806-8934342d18cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_df = wikipedia_df.withColumn('value', struct('value.*', \n",
    "                                       (col('value.length.new') - col('value.length.old')).alias('BYTECHANGE')))\n",
    "bots_df = wikipedia_df.filter((col('value.bot') == True) & (col('value.length.new').isNotNull()) & (col('value.length.new').isNotNull()))\n",
    "no_bots_df = wikipedia_df.filter((col('value.bot') == False) & (col('value.length.new').isNotNull()) & (col('value.length.new').isNotNull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50a904-f6d6-459f-9f65-05d8afba89bf",
   "metadata": {},
   "source": [
    "### Parsing value data to send back to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ac8432e-50af-4083-85a6-e6e705d713d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_avro(col, config):\n",
    "    \"\"\"\n",
    "    avro serialize\n",
    "    :param col (PySpark column / str): column name \"key\" or \"value\"\n",
    "    :param config (za.co.absa.abris.config.ToAvroConfig): abris config, generated from abris_config helper function\n",
    "    :return: PySpark Column\n",
    "    \"\"\"\n",
    "    jvm_gateway = SparkContext._active_spark_context._gateway.jvm\n",
    "    abris_avro = jvm_gateway.za.co.absa.abris.avro\n",
    "\n",
    "    return Column(abris_avro.functions.to_avro(_to_java_column(col), config))\n",
    "\n",
    "def to_avro_abris_config(config_map, topic, is_key):\n",
    "    \"\"\"\n",
    "    Create to avro abris config with a schema url\n",
    "\n",
    "    :param config_map (dict[str, str]): configuration map to pass to the serializer, ex: {'schema.registry.url': 'http://localhost:8081'}\n",
    "    :param topic (str): kafka topic\n",
    "    :param is_key (bool): boolean\n",
    "    :return: za.co.absa.abris.config.ToAvroConfig\n",
    "    \"\"\"\n",
    "    jvm_gateway = SparkContext._active_spark_context._gateway.jvm\n",
    "    scala_map = jvm_gateway.PythonUtils.toScalaMap(config_map)\n",
    "\n",
    "    return jvm_gateway.za.co.absa.abris.config \\\n",
    "        .AbrisConfig \\\n",
    "        .toConfluentAvro() \\\n",
    "        .downloadSchemaByLatestVersion() \\\n",
    "        .andTopicNameStrategy(topic, is_key) \\\n",
    "        .usingSchemaRegistry(scala_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11f6b97c-fe09-4d1d-9eb3-35aa8eb8f3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_bots_df = no_bots_df.select(col('value'))\n",
    "bots_df = bots_df.select(col('value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1f1eb68-8b7d-4f56-8c85-546dea606333",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_avro_abris_settings = to_avro_abris_config(confluent_config, 'WIKIPEDIABOT', False)\n",
    "no_bots_df = no_bots_df.withColumn('value', to_avro(col('value'), to_avro_abris_settings))\n",
    "bots_df = bots_df.withColumn('value', to_avro(col('value'), to_avro_abris_settings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe4d255-e310-4937-8e7c-687369bd06b5",
   "metadata": {},
   "source": [
    "### Writing back to Kafka "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "def7683a-eb5d-4f34-921c-ca316229be35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f8cf0e43730>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bots_df.writeStream.format('kafka')\\\n",
    "    .option(\"kafka.bootstrap.servers\", 'localhost:12091') \\\n",
    "    .option(\"checkpointLocation\", \"../checkpoints/\") \\\n",
    "    .option(\"topic\", \"WIKIPEDIABOT\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8b1052b-2a0f-43ef-a3fc-c38fdafcc1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f8cf0fbc070>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_bots_df.writeStream.format('kafka')\\\n",
    "    .option(\"kafka.bootstrap.servers\", 'localhost:12091') \\\n",
    "    .option(\"checkpointLocation\", \"../checkpoints/\") \\\n",
    "    .option(\"topic\", \"WIKIPEDIANOBOT\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4388b97-2973-4ed3-8636-c7d20c67ea3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
