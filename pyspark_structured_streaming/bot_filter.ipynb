{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a035fe-2894-4bcd-ab87-b541eb31a443",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastavro in /opt/conda/lib/python3.10/site-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install fastavro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1be2bd04-ee38-4c6b-ad52-f2b15918adc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/19 18:46:13 WARN Utils: Your hostname, vplentz-computer resolves to a loopback address: 127.0.1.1; using 192.168.0.104 instead (on interface wlp2s0)\n",
      "22/06/19 18:46:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.2.1\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.15\n",
      "Branch HEAD\n",
      "Compiled by user hgao on 2022-01-20T19:26:14Z\n",
      "Revision 4f25b3f71238a00508a356591553f2dfa89f8290\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52332b0f-9e50-4bb7-8aa7-872bc5b02ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--repositories https://packages.confluent.io/maven --packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1,io.confluent:kafka-schema-registry-client:7.1.1,io.confluent:kafka-avro-serializer:7.1.1,za.co.absa:abris_2.12:6.3.0 pyspark-shell\n"
     ]
    }
   ],
   "source": [
    "# za.co.absa\" %% \"abris\" % \"4.0.1\n",
    "import os\n",
    "kafka_deps = 'org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1'\n",
    "confluent_schema_deps = 'io.confluent:kafka-schema-registry-client:7.1.1,' + 'io.confluent:kafka-avro-serializer:7.1.1'\n",
    "abris_deps = 'za.co.absa:abris_2.12:6.3.0' \n",
    "#jars_path = \",\".join(['/home/jovyan/work/pyspark_structured_streaming/jars/jar_files'+file for file in os.listdir('/home/jovyan/work/pyspark_structured_streaming/jars/jar_files/')])\n",
    "spark_deps  = f\"--repositories https://packages.confluent.io/maven --packages {kafka_deps},{confluent_schema_deps},{abris_deps} pyspark-shell\"\n",
    "print(spark_deps)\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = spark_deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145421e9-1d55-4885-8666-5f1ea447ea5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c3f08d-47ff-47a8-90f8-52b0d77bb49d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, udf, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa83255c-b61a-49f5-bb21-0384cd8d3eef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_context = SparkSession.builder.appName('processingBots').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a5c243e-1a6a-4ce5-8732-48e047a8524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.column import Column, _to_java_column\n",
    "\n",
    "def from_avro(col, config):\n",
    "    \"\"\"\n",
    "    avro deserialize\n",
    "\n",
    "    :param col (PySpark column / str): column name \"key\" or \"value\"\n",
    "    :param config (za.co.absa.abris.config.FromAvroConfig): abris config, generated from abris_config helper function\n",
    "    :return: PySpark Column\n",
    "    \"\"\"\n",
    "    jvm_gateway = SparkContext._active_spark_context._gateway.jvm\n",
    "    abris_avro = jvm_gateway.za.co.absa.abris.avro\n",
    "\n",
    "    return Column(abris_avro.functions.from_avro(_to_java_column(col), config))\n",
    "\n",
    "def from_avro_abris_config(config_map, topic, is_key):\n",
    "    \"\"\"\n",
    "    Create from avro abris config with a schema url\n",
    "\n",
    "    :param config_map (dict[str, str]): configuration map to pass to deserializer, ex: {'schema.registry.url': 'http://localhost:8081'}\n",
    "    :param topic (str): kafka topic\n",
    "    :param is_key (bool): boolean\n",
    "    :return: za.co.absa.abris.config.FromAvroConfig\n",
    "    \"\"\"\n",
    "    jvm_gateway = SparkContext._active_spark_context._gateway.jvm\n",
    "    scala_map = jvm_gateway.PythonUtils.toScalaMap(config_map)\n",
    "\n",
    "    return jvm_gateway.za.co.absa.abris.config \\\n",
    "        .AbrisConfig \\\n",
    "        .fromConfluentAvro() \\\n",
    "        .downloadReaderSchemaByLatestVersion() \\\n",
    "        .andTopicNameStrategy(topic, is_key) \\\n",
    "        .usingSchemaRegistry(scala_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a6296-cac2-4620-b881-2c74a31f18ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5e7e787-834f-4aa9-92d7-b4b485b29e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_offsets = 'earliest'\n",
    "\n",
    "wikipedia_df = spark_context.readStream.format('kafka')\\\n",
    "    .option('kafka.bootstrap.servers', 'localhost:12091')\\\n",
    "    .option('subscribe', 'wikipedia.parsed')\\\n",
    "    .option(\"startingOffsets\", starting_offsets)\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68a4f5fd-bd50-4e2e-8c85-d473d0be1bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/pyspark_structured_streaming\n"
     ]
    }
   ],
   "source": [
    "!echo ${PWD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "718e94da-9c6e-4d97-b78d-c49ae663e892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wikipedia_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "012da78f-74dc-4bfd-acd3-a7f3114aceef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fastavro\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d9b3f49-56d1-440f-8c62-64560b327289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "819f6f3a-18b0-4fd0-ba7f-e6a82b7b4d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "confluent_config = {\n",
    "    'schema.registry.url':'https://localhost:8085',\n",
    "    #\"schema.registry.ssl.certificate.location\": \"/home/jovyan/work/scripts/security/snakeoil-ca-1.crt\",\n",
    "    \"schema.registry.ssl.truststore.location\" :\"/home/jovyan/work/scripts/security/kafka.schemaregistry.truststore.jks\",\n",
    "    \"schema.registry.ssl.truststore.password\": 'confluent',\n",
    "    \"schema.registry.ssl.keystore.location\": '/home/jovyan/work/scripts/security/kafka.schemaregistry.keystore.jks',\n",
    "    \"schema.registry.ssl.keystore.password\" : \"confluent\",\n",
    "    \"basic.auth.credentials.source\": \"USER_INFO\",\n",
    "    \"basic.auth.user.info\": \"superUser:superUser\"\n",
    "} \n",
    "from_avro_abris_settings = from_avro_abris_config(confluent_config, 'wikipedia.parsed', False)\n",
    "wikipedia_df = wikipedia_df.withColumn('value', from_avro(col('value'), from_avro_abris_settings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbcac7a6-e417-4648-9a2d-8494d82d4735",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- bot: boolean (nullable = true)\n",
      " |    |-- comment: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- length: struct (nullable = true)\n",
      " |    |    |-- new: long (nullable = true)\n",
      " |    |    |-- old: long (nullable = true)\n",
      " |    |-- log_action: string (nullable = true)\n",
      " |    |-- log_action_comment: string (nullable = true)\n",
      " |    |-- log_id: long (nullable = true)\n",
      " |    |-- log_type: string (nullable = true)\n",
      " |    |-- meta: struct (nullable = false)\n",
      " |    |    |-- domain: string (nullable = true)\n",
      " |    |    |-- dt: timestamp (nullable = false)\n",
      " |    |    |-- id: string (nullable = false)\n",
      " |    |    |-- request_id: string (nullable = true)\n",
      " |    |    |-- stream: string (nullable = false)\n",
      " |    |    |-- uri: string (nullable = true)\n",
      " |    |-- minor: boolean (nullable = true)\n",
      " |    |-- namespace: long (nullable = true)\n",
      " |    |-- parsedcomment: string (nullable = true)\n",
      " |    |-- patrolled: boolean (nullable = true)\n",
      " |    |-- revision: struct (nullable = true)\n",
      " |    |    |-- new: long (nullable = true)\n",
      " |    |    |-- old: long (nullable = true)\n",
      " |    |-- server_name: string (nullable = true)\n",
      " |    |-- server_script_path: string (nullable = true)\n",
      " |    |-- server_url: string (nullable = true)\n",
      " |    |-- timestamp: long (nullable = true)\n",
      " |    |-- title: string (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |    |-- user: string (nullable = true)\n",
      " |    |-- wiki: string (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wikipedia_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa37a975-2454-492c-b5e6-f7c913aa9804",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fceae098100>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_df.writeStream.format('console').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8b1052b-2a0f-43ef-a3fc-c38fdafcc1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f4017d104f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_df.filter(col('value.bot') != True).\\\n",
    "    .withColumn('BYTECHANGE', (col('length.new') - col('length.old'))).\n",
    "    .select('*').writeStream.format('kafka')\\\n",
    "    .trigger(processingTime='5 minutes')\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:8091\") \\\n",
    "    .option(\"checkpointLocation\", \"../checkpoints/\") \\\n",
    "        .option(\"topic\", \"WIKIPEDIABOT\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184b1419-fe90-4805-ba0a-14f7d810ee65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
